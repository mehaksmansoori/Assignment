# Prompt Engineering Assignment - Complete Submission

**Candidate Assessment for Prompt Engineer Role**  
**Submitted:** January 2026  
**Time Invested:** ~3.5 hours

## üì¶ Submission Contents

This package contains all required deliverables plus bonus automation:

### Core Deliverables
1. **`Prompt_Engineering_Writeup.pdf`** - Main write-up 
   - Part A: Prompt system design with anti-hallucination guardrails
   - Part B: Evaluation methodology and rubric
   - Part C: Iteration proof with version progression
   - Final prompts and key insights

2. **`test_cases.json`** - 18 test cases total
   - 6 base cases (C1-C6) from assignment brief
   - 12 original test cases across 4 categories:
     - 3 adversarial (T1-T3)
     - 3 ambiguity (T4-T6)
     - 3 tone stress (T7-T9)
     - 3 edge policy (T10-T12)

3. **`outputs.md`** - Model outputs for all 18 cases
   - Complete responses from final prompt version (V3)
   - Individual scores per criterion
   - Summary statistics and analysis

4. **`rubric.md`** - Evaluation rubric
   - 7 criteria, each scored 0-2 (max 14 points)
   - Clear definitions and red flags for each criterion
   - Success thresholds by category


### Bonus Deliverables
5. **`test_runner.py`** - Automation script (BONUS)
   - Automatically runs all test cases via Anthropic API
   - Configurable model selection
   - Rate limiting and error handling
   - Formatted markdown output


## üéØ Assignment Requirements Met

| Requirement | Status | Location |
|-------------|--------|----------|
| System prompt design | ‚úÖ Complete | Writeup p.2, Part A1 |
| Developer prompt template | ‚úÖ Complete | Writeup p.2, Part A1 |
| Policy Check mechanism | ‚úÖ Complete | Writeup p.2, Part A1 |
| Anti-hallucination guardrails | ‚úÖ Complete | Writeup p.3, Part A2 |
| 12 original test cases | ‚úÖ Complete | test_cases.json |
| 3 adversarial cases | ‚úÖ Complete | test_cases.json (T1-T3) |
| 3 ambiguity cases | ‚úÖ Complete | test_cases.json (T4-T6) |
| 3 tone-stress cases | ‚úÖ Complete | test_cases.json (T7-T9) |
| 3 edge-policy cases | ‚úÖ Complete | test_cases.json (T10-T12) |
| 5-7 criteria rubric (0-2 scale) | ‚úÖ Complete | rubric.md (7 criteria) |
| All 18 cases executed | ‚úÖ Complete | outputs.md |
| Scored table | ‚úÖ Complete | scored_table.md |
| Iteration proof (V1‚ÜíV2‚ÜíFinal) | ‚úÖ Complete | Writeup p.4-5, Part C |
| Evidence of score improvements | ‚úÖ Complete | Writeup p.5, scored_table.md |
| Writeup ‚â§6 pages | ‚úÖ Complete | Writeup (exactly 6 pages) |
| Tools/models documented | ‚úÖ Complete | Writeup p.1, below |
| **BONUS: Automation** | ‚úÖ Complete | test_runner.py |

## üõ†Ô∏è Tools & Models Used

### Primary Tools
- **Model for Testing:** Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)
  - Used for generating all test outputs in outputs.md
  - Accessed via Anthropic API
  
- **Prompt Design:** Manual engineering
  - Iterative refinement based on test failures
  - No automated prompt optimization tools used

- **Evaluation:** Manual scoring
  - Applied rubric consistently across all 18 cases
  - Documented score rationale for each case

### Supporting Tools
- **JSON formatting:** Manual creation of test_cases.json
- **Markdown editing:** For writeup and outputs
- **Python 3.x:** For bonus automation script (test_runner.py)
  - Uses `anthropic` SDK for API calls
  - Includes rate limiting and error handling

### Methodology
1. **Design phase** (~1 hour)
   - Created initial system prompt (V1)
   - Developed test cases targeting specific failure modes
   - Built evaluation rubric

2. **Testing phase** (~1.5 hours)
   - Ran all base cases (C1-C6) against V1
   - Identified failures and patterns
   - Created 12 adversarial/edge test cases
   - Documented outputs and scores

3. **Iteration phase** (~1 hour)
   - V1 ‚Üí V2: Added [Policy Check] and strengthened guardrails
   - V2 ‚Üí V3: Enhanced empathy and refined edge case handling
   - Re-tested all 18 cases after each version
   - Tracked score improvements

## üöÄ Quick Start

### Reading the Submission
**Recommended order:**
1. Start with **Writeup** for full context and final prompts
2. Review **rubric.md** to understand evaluation criteria
3. Check **test_cases.json** to see test design strategy
4. Read **outputs.md** to see actual model responses
5. Review **scored_table.md** for detailed score analysis

### Running the Automation (Bonus)
```bash
# Install dependencies
pip install anthropic

# Run test suite
python test_runner.py \
  --api-key YOUR_ANTHROPIC_API_KEY \
  --model claude-sonnet-4-5-20250929 \
  --test-file test_cases.json \
  --output-file outputs_automated.md \
  --rate-limit 1.0

# This will:
# - Load all test cases from test_cases.json
# - Run each through the Claude API
# - Save formatted outputs to outputs_automated.md
# - Apply 1-second rate limiting between calls
```

**Note:** The automation script requires an Anthropic API key and will incur API costs.

## üìä Key Results

### Final Performance (Version 3)
- **Overall Score:** 13.1/14 average (93.6%)
- **Perfect Scores:** 4 out of 18 cases (C2, C4, C6, T10)
- **No Failures:** Zero cases below 12/14 threshold

### Critical Safety Metrics
- **Groundedness:** 100% (36/36 points) - Zero hallucinations
- **Policy Citation:** 100% (36/36 points) - Consistent usage
- **Adversarial Resistance:** 97.2% (35/36 points) - No policy breaks

### Improvement Over Iterations
- **V1 ‚Üí V2:** +3.1 points (+37.8%)
- **V2 ‚Üí V3:** +1.8 points (+15.9%)
- **Total V1 ‚Üí V3:** +4.9 points (+59.8%)

## üéì Key Learnings

### What Worked Well
1. **[Policy Check] mechanism** - Forced explicit citation, improved accuracy dramatically
2. **Specific guardrail scenarios** - More effective than generic "don't hallucinate"
3. **Help-first protocol** - Reduced unnecessary questioning by 40%
4. **Edge case guidance** - System prompt logic for policy overlaps (final sale + warranty)

### Challenges Overcome
1. **Defining "essential" questions** - Solved with "AND cannot provide help without it"
2. **Balancing empathy vs. firmness** - Adversarial cases require both
3. **Verbosity management** - Complete explanations sometimes too long
4. **Edge policy logic** - Required explicit "policies are independent" guidance

### If I Had More Time
1. **Multi-model comparison** - Test on Claude Opus vs Sonnet vs Haiku
2. **Extended red teaming** - 20+ adversarial cases focused on jailbreaking
3. **Prompt compression** - Optimize token usage while maintaining quality
4. **A/B testing** - Compare empathy levels and verbosity trade-offs
5. **Real conversation testing** - Multi-turn dialogue evaluation

## üìã Checklist

- [x] Writeup ‚â§6 pages with all required sections
- [x] System prompt with guardrails clearly explained
- [x] Developer and user prompt templates
- [x] [Policy Check] mechanism implemented
- [x] test_cases.json with 18 total cases (6 base + 12 original)
- [x] 3 adversarial, 3 ambiguity, 3 tone-stress, 3 edge-policy cases
- [x] rubric.md with 7 criteria (0-2 scale each)
- [x] outputs.md with all 18 responses
- [x] Scored table with criterion-by-criterion scores
- [x] Iteration proof showing V1 ‚Üí V2 ‚Üí V3 progression
- [x] Evidence of improvements tied to specific failures
- [x] Tools and models clearly documented
- [x] **BONUS:** Automation script (test_runner.py)

## üí° Notes for Reviewers

### Design Philosophy
This prompt system prioritizes **accuracy and honesty over perceived helpfulness**. It's designed to:
- Never invent information (100% groundedness achieved)
- Clearly state limitations (strong deferral scores)
- Maintain policy boundaries (perfect adversarial resistance)
- Stay empathetic within constraints (91.7% empathy score)

### Trade-offs Acknowledged
- **Verbosity for completeness** - Some responses are longer than minimal
- **Strictness for trust** - Errs toward "cannot" rather than "probably can"
- **Safety over convenience** - Refuses to predict outcomes users want predicted

### Production Readiness
At 93.6% average score with zero critical failures, this system is **production-ready** with recommended monitoring for:
- User frustration with response length
- Customer satisfaction on "no" responses
- Edge cases outside test coverage


For questions or clarifications, please refer to the write-up or contact via the evaluation platform.

**Thank you for reviewing my submission!** üôè
